# Impact Measurement User Stories - Pitfall Prevention Focus
## Methodology-Driven Design to Prevent Common Measurement Traps

### Overview
These user stories are designed around preventing the 5 major pitfalls identified in impact measurement practice, while leveraging our IRIS+ dataset advantage to surface the right indicators at decision points.

**Core Philosophy: Foundation-First, Decision-Driven, Learning-Oriented**

**Anti-Pitfall Framework:**
1. **Foundation & Theory First** - Prevent jumping to metrics without context
2. **Decision-Driven Design** - "What decisions will this data inform?"
3. **Impact vs Activity Intelligence** - AI distinguishes outputs/outcomes/impacts
4. **Proxy Detection & Alternatives** - Flag proxy metrics, suggest direct measures
5. **Learning vs Proving Culture** - Support improvement over justification

**Proven Methodology Integration:**
- Three-lens approach: Quantitative + Qualitative + Expert Opinion
- Contribution vs Attribution honesty
- Minimum viable measurement design
- Progressive complexity based on organizational readiness

---

## ðŸ›¡ï¸ PHASE 1: Foundation-First Measurement Design
*Prevent "jumping to metrics" - establish decision context and impact logic*

### Decision-Driven Foundation Capture

#### Theory of Change Foundation with Flexible Entry
**As a program director**, I want to quickly upload my existing theory of change OR be supported to create one if I don't have one so that I get the foundation needed for good measurement without unnecessary barriers.

**Acceptance Criteria:**
- System asks: "Do you have a theory of change or strategy document?"
- **If YES:** Upload documents (PDF, Word, PowerPoint, images) with AI extraction
- **If NO:** "No problem! I'll help you create one" - guided 15-20 minute conversation
- **If PARTIAL:** "I have some pieces" - hybrid approach combining uploads with guided development
- Theory of change can be updated and refined over time
- Foundation readiness score shows what's complete and what needs work
- Users can access basic indicators with minimal foundation, advanced features require more complete foundation

**Technical Requirements:**
- `POST /api/v1/organizations/foundation/upload-documents` 
- `POST /api/v1/organizations/foundation/guided-creation`
- `PUT /api/v1/organizations/foundation/update`
- `GET /api/v1/organizations/foundation/readiness-score`
- Flexible foundation capture (upload OR create OR hybrid)
- Progressive access based on foundation completeness
- Theory of change versioning and update workflow

---

#### Decision Mapping with Update Capability
**As an M&E lead**, I want to identify and update what specific decisions our measurement will inform so that our measurement system evolves with our changing needs.

**Acceptance Criteria:**
- AI guides through decision identification: "What decisions will this data help you make?"
- Decision mapping can be updated as organizational needs change
- System tracks decision evolution over time for learning
- Maps decisions to required evidence and measurement approaches
- Creates minimum viable indicator framework based on current decision requirements
- Dashboard shows decision-to-indicator mapping for review and updates

**Technical Requirements:**
- `POST /api/v1/decisions/mapping-session`
- `PUT /api/v1/decisions/update-mapping`
- `GET /api/v1/decisions/evolution-history`
- `GET /api/v1/decisions/minimum-viable-indicators`
- Decision mapping versioning and history
- Decision evolution tracking for analytics
- Real-time decision-to-indicator mapping updates

---

#### Organizational Context Synthesis
**As a system user**, I want the AI to understand and remember my organization's context, constraints, and priorities so that every recommendation and suggestion is relevant to our specific situation.

**Acceptance Criteria:**
- System captures: sector, geography, scale, capacity, existing systems
- Synthesizes constraints: budget, timeline, skills, technology
- Stores priority stakeholders and reporting requirements
- Context influences all subsequent AI interactions
- Context can be updated and refined over time

**Technical Requirements:**
- Organizational context data model
- Context synthesis algorithms
- Priority weighting systems
- Context versioning and history
- Integration with all recommendation engines

---

#### Foundation Readiness Assessment
**As a user beginning impact measurement**, I want to understand what foundational elements I need before diving into indicator selection so that I set myself up for measurement success.

**Acceptance Criteria:**
- Assessment of theory of change completeness
- Evaluation of organizational readiness for measurement
- Identification of prerequisite work needed
- Guidance on addressing foundational gaps
- Clear go/no-go decision point for proceeding

**Technical Requirements:**
- `GET /api/v1/organizations/foundation/readiness-assessment`
- Readiness scoring algorithms
- Gap prioritization and sequencing
- Prerequisite work templates
- Foundation completion tracking

---

## ðŸ¤– PHASE 2: AI-Powered Pitfall Prevention
*Real-time guidance against common measurement traps*

### Activity vs Impact Intelligence

#### Output/Outcome/Impact Level Detection
**As an M&E practitioner**, I want the AI to actively warn me when I'm selecting activity measures instead of impact measures so that I avoid the "mistaking activity for impact" pitfall.

**Acceptance Criteria:**
- AI flags when indicator selection is output-heavy with warning messages
- "This measures what you do, not what changes" warnings for activity indicators
- Automatic suggestions for outcome alternatives to every output indicator
- Red/amber/green indicator balance scoring for outputs/outcomes/impacts
- Forced review step when selecting >60% output indicators

**Technical Requirements:**
- `POST /api/v1/indicators/activity-impact-analysis`
- `GET /api/v1/indicators/outcome-alternatives`
- Real-time activity vs impact classification
- Outcome alternative suggestion engine for every output
- Portfolio balance scoring and warning system
- Impact level visualization with warning highlights

---

#### Proxy Metric Detection & Direct Alternatives
**As a program manager**, I want the AI to identify when I'm selecting proxy metrics and suggest direct measurement alternatives so that I avoid measuring stand-ins instead of actual change.

**Acceptance Criteria:**
- AI identifies proxy metrics with clear explanations: "This measures X as a proxy for Y"
- Suggests direct measurement alternatives: "Instead of measuring attendance, measure engagement"
- Provides triangulation suggestions when proxies are necessary
- Flags common proxy traps: attendance for engagement, satisfaction for behavior change
- Validates that direct measurement is feasible for organizational capacity

**Technical Requirements:**
- `POST /api/v1/indicators/proxy-detection`
- `GET /api/v1/indicators/direct-alternatives`
- Proxy metric identification algorithms
- Direct measurement alternative suggestion engine
- Triangulation method recommendations
- Feasibility assessment for direct vs proxy measurement

---

### Over-Engineering Prevention

#### Minimum Viable Measurement Design
**As an impact measurement lead**, I want the AI to prevent me from building an overly complex measurement system by focusing on the minimum viable data set needed for my decisions.

**Acceptance Criteria:**
- AI calculates measurement burden score and warns when excessive
- "Less is more" recommendations when indicator count is high
- Decision mapping: "Which decisions need this specific indicator?"
- Flags indicators that don't map to specific decisions
- Suggests indicator consolidation opportunities

**Technical Requirements:**
- `POST /api/v1/indicators/burden-assessment`
- `GET /api/v1/indicators/consolidation-opportunities`
- Measurement burden calculation algorithms
- Decision-to-indicator mapping validation
- Complexity warning system with simplification suggestions
- Minimum viable measurement recommendations

---

#### Three-Lens Measurement Validation
**As an M&E coordinator**, I want the AI to ensure I have quantitative evidence + qualitative insight + expert opinion in my measurement approach so that I get a complete picture of impact.

**Acceptance Criteria:**
- AI validates measurement portfolio has all three lenses represented
- Warnings when approach is too heavily quantitative or qualitative
- Suggests expert opinion integration methods
- Templates for lived experience data collection
- Triangulation recommendations for comprehensive understanding

**Technical Requirements:**
- `POST /api/v1/measurement/three-lens-validation`
- `GET /api/v1/measurement/lens-balance-check`
- Three-lens portfolio analysis
- Qualitative method suggestion engine
- Expert opinion integration templates
- Triangulation planning tools

---

### Quantitative/Qualitative Balance

#### Mixed Methods Indicator Portfolio
**As an M&E coordinator**, I want guidance on balancing quantitative and qualitative indicators so that I capture both the scale and depth of our impact.

**Acceptance Criteria:**
- AI recommends mix of quantitative and qualitative measures
- Explains when qualitative data is essential for understanding
- Suggests qualitative methods for context and stories
- Ensures quantitative measures have qualitative validation
- Provides templates for mixed methods data collection

**Technical Requirements:**
- Mixed methods portfolio optimization
- Qualitative method suggestion engine
- Data triangulation planning tools
- Mixed methods templates and guides
- Balance scoring and recommendations

---

### Custom Indicator Development

#### IRIS+ Gap Identification & Custom Indicator Creation
**As an M&E lead**, I want to identify where IRIS+ doesn't cover my specific impact areas and create custom indicators that align with the framework's methodology so that I measure what truly matters for my unique context.

**Acceptance Criteria:**
- AI analyzes theory of change for gaps not covered by IRIS+ indicators
- Provides templates for developing custom indicators following IRIS+ methodology
- Validates custom indicators against SMART criteria and impact levels
- Suggests how custom indicators relate to existing IRIS+ strategic goals
- Ensures custom indicators maintain methodological rigor

**Technical Requirements:**
- `POST /api/v1/indicators/analyze-iris-gaps`
- Custom indicator development templates
- IRIS+ methodology validation for custom indicators
- Strategic goal alignment suggestions
- Custom indicator quality assessment

---

#### Community-Defined Success Indicators
**As a program participant/beneficiary**, I want to define what success means in my own terms and have those perspectives translated into measurable indicators so that measurement reflects my lived experience and priorities.

**Acceptance Criteria:**
- Community engagement tools for participatory indicator development
- Translation of community language into measurable indicators
- Integration of cultural and contextual success definitions
- Validation that community indicators complement IRIS+ framework
- Templates for ongoing community input on indicator relevance

**Technical Requirements:**
- `POST /api/v1/indicators/community-defined`
- Participatory indicator development tools
- Community language to metric translation
- Cultural context integration
- Community validation workflows

---

#### Innovation & Pilot Program Indicators
**As an innovation manager**, I want to create indicators for experimental or pilot programs where standard frameworks may not apply so that I can measure learning and adaptation in early-stage initiatives.

**Acceptance Criteria:**
- Framework for developing indicators for experimental work
- Learning-focused indicators beyond traditional outcome measures
- Adaptation and iteration tracking indicators
- Risk and failure measurement alongside success metrics
- Transition pathway from pilot to scale indicators

**Technical Requirements:**
- Experimental program indicator templates
- Learning and adaptation measurement frameworks
- Risk assessment indicator development
- Pilot-to-scale transition planning
- Innovation metric validation

---

#### Sector-Specific Custom Indicators
**As a sector specialist** (e.g., mental health, disability rights, indigenous communities), I want to develop indicators that capture sector-specific impacts that may not be well-represented in IRIS+ so that I measure outcomes that matter to my specialized field.

**Acceptance Criteria:**
- Sector-specific indicator libraries and templates
- Integration with relevant professional standards and frameworks
- Validation against sector best practices and research
- Connection to broader impact frameworks beyond IRIS+
- Peer review and sector expert validation options

**Technical Requirements:**
- `POST /api/v1/indicators/sector-specific`
- Sector indicator template libraries
- Professional framework integration (medical, educational, etc.)
- Sector expert validation networks
- Cross-framework alignment tools

---

#### Organization-Specific Context Indicators
**As a program director with unique organizational approaches**, I want to create indicators that capture my organization's distinctive methods and impacts so that I measure what makes our approach special and effective.

**Acceptance Criteria:**
- Analysis of organizational uniqueness and distinctive approaches
- Custom indicator development for proprietary methods
- Validation that custom indicators don't duplicate existing IRIS+ measures
- Integration between custom and standard indicators in reporting
- Templates for explaining custom indicators to stakeholders

**Technical Requirements:**
- Organizational uniqueness analysis tools
- Proprietary method indicator development
- Duplication detection with IRIS+ database
- Integrated reporting template generation
- Stakeholder explanation templates

---

## ðŸŽ¯ PHASE 3: Context-Aware IRIS+ Discovery
*Surface right indicators at decision points using our data advantage*

### Decision-Point Contextual Recommendations

#### Smart IRIS+ Surfacing at Decision Moments
**As a program manager**, I want the AI to surface relevant IRIS+ indicators precisely when I'm making specific decisions so that I leverage our comprehensive dataset advantage at the right moments.

**Acceptance Criteria:**
- AI detects decision context and surfaces relevant IRIS+ indicators
- Contextual recommendations based on sector, intervention type, and organizational capacity
- "Organizations like yours typically measure..." insights
- Real-time indicator suggestions during theory development
- Decision-specific indicator filtering and ranking

**Technical Requirements:**
- `POST /api/v1/recommendations/contextual-discovery`
- `GET /api/v1/indicators/decision-specific`
- Contextual recommendation engine using hybrid vector + structured content
- Decision moment detection and trigger system
- Organizational similarity matching for "like yours" insights
- Real-time IRIS+ filtering based on conversation context

---

#### Sector-Specific Pitfall Prevention
**As a mental health/disability/social inclusion practitioner**, I want sector-specific guidance about measurement pitfalls and appropriate indicators so that I avoid common traps in my complex field.

**Acceptance Criteria:**
- Sector-specific pitfall warnings (e.g., "In mental health, attribution is particularly complex")
- Field-appropriate indicator suggestions that acknowledge complexity
- Contribution vs attribution guidance specific to sector challenges
- Templates for discussing multi-causal change in complex fields
- Realistic expectation setting for different intervention types

**Technical Requirements:**
- `GET /api/v1/sectors/:sector/pitfall-guidance`
- `POST /api/v1/indicators/sector-appropriate`
- Sector-specific pitfall knowledge base
- Field-appropriate indicator filtering
- Complexity acknowledgment templates
- Sector-specific contribution language suggestions

---

### Custom Indicator Data Collection Design

#### Custom Indicator Data Collection Planning
**As a data manager**, I want to design data collection methods for custom indicators that maintain the same quality standards as IRIS+ indicators so that all our measurement is equally credible and reliable.

**Acceptance Criteria:**
- AI suggests appropriate data collection methods for custom indicator types
- Quality standards alignment with IRIS+ methodology requirements
- Integration with existing data collection workflows
- Validation protocols specific to custom indicator characteristics
- Training materials for staff collecting custom indicator data

**Technical Requirements:**
- `POST /api/v1/indicators/custom/:id/data-collection-design`
- Custom indicator data method recommendation engine
- Quality standard validation for custom indicators
- Integration planning with existing workflows
- Custom indicator training material generation

---

#### Custom Indicator Validation & Reliability Testing
**As an M&E practitioner**, I want to test and validate my custom indicators before full implementation so that I can ensure they measure what I intend and produce reliable results.

**Acceptance Criteria:**
- Pilot testing protocols for custom indicators
- Reliability and validity assessment tools
- Comparison testing against proxy measures or existing data
- Stakeholder feedback integration on indicator meaningfulness
- Refinement recommendations based on testing results

**Technical Requirements:**
- Custom indicator pilot testing frameworks
- Reliability testing tools and calculations
- Validity assessment protocols
- Stakeholder feedback collection systems
- Indicator refinement recommendation engine

---

### Multi-Source Data Integration

#### Data Source Mapping & Integration
**As a data manager**, I want to map our selected indicators to available data sources and create integrated collection workflows so that we minimize burden while maximizing data quality.

**Acceptance Criteria:**
- AI identifies existing data sources for each indicator
- Maps external data sources (government, industry benchmarks)
- Creates integrated collection calendars to minimize burden
- Suggests data collection efficiencies and automation
- Plans for data source triangulation and validation

**Technical Requirements:**
- `POST /api/v1/data-sources/mapping-analysis`
- External data source API integrations
- Collection calendar optimization
- Data triangulation planning
- Source reliability assessment

---

#### Data Quality Assurance Design
**As a data quality manager**, I want to design validation and quality controls for each indicator so that we collect reliable data that stakeholders can trust.

**Acceptance Criteria:**
- Quality control protocols specific to each indicator
- Validation rules and outlier detection setup
- Inter-rater reliability protocols for qualitative data
- Data verification and spot-checking procedures
- Quality scoring and improvement feedback loops

**Technical Requirements:**
- `POST /api/v1/indicators/quality-assurance-setup`
- Quality control protocol generation
- Validation rule configuration
- Reliability assessment tools
- Quality improvement feedback systems

---

## ðŸ“Š PHASE 4: Learning-Driven Analytics
*Support improvement over proving - honest impact assessment*

### Contribution vs Attribution Honesty

#### Realistic Impact Claim Guidance
**As a program evaluator**, I want AI to guide me toward honest contribution claims rather than overstated attribution so that I build credibility rather than making unrealistic impact claims.

**Acceptance Criteria:**
- AI evaluates evidence strength and suggests contribution vs attribution language
- Identifies confounding factors: "Government also improved teacher training"
- Provides honest impact language templates: "Our work contributed to X alongside Y factors"
- Flags overstated claims and suggests more credible alternatives
- Acknowledges complexity and multi-causal change

**Technical Requirements:**
- `POST /api/v1/analysis/contribution-assessment`
- `GET /api/v1/language/honest-impact-claims`
- Evidence strength assessment for attribution vs contribution
- Confounding factor identification algorithms
- Honest impact language suggestion engine
- Credibility-building claim templates

---

#### Learning-Focused Question Generation
**As an impact analyst**, I want the AI to generate learning questions from our measurement data so that we focus on improvement rather than just reporting results.

**Acceptance Criteria:**
- AI generates learning questions from data patterns: "Why did X work better than Y?"
- Focuses on program improvement insights rather than just reporting
- Suggests areas for deeper investigation based on unexpected results
- Creates learning review agendas from measurement findings
- Prompts reflection on assumptions that data confirms or challenges

**Technical Requirements:**
- `POST /api/v1/analysis/learning-questions`
- `GET /api/v1/insights/improvement-suggestions`
- Learning question generation from data patterns
- Program improvement suggestion engine
- Learning review template generation
- Assumption testing analysis

---

### Contextual Interpretation

#### External Factor Integration
**As a program analyst**, I want to understand how external factors may be influencing our results so that I can interpret our impact findings in proper context.

**Acceptance Criteria:**
- AI identifies relevant external factors (economic, political, environmental)
- Suggests data sources for external factor tracking
- Provides frameworks for contextual interpretation
- Templates for discussing external influences in reports
- Historical context and trend analysis

**Technical Requirements:**
- External factor identification algorithms
- Context data source suggestions
- Interpretation framework templates
- Historical trend analysis tools
- Contextual reporting templates

---

#### Positive & Negative Impact Assessment
**As an impact evaluator**, I want to systematically identify both positive and negative impacts of our work so that we have a complete and honest picture of our effects.

**Acceptance Criteria:**
- AI prompts consideration of potential negative impacts
- Frameworks for identifying unintended consequences
- Templates for balanced impact assessment
- Stakeholder feedback integration on unintended effects
- Improvement recommendations for negative impacts

**Technical Requirements:**
- Unintended consequence identification prompts
- Balanced assessment frameworks
- Stakeholder feedback integration
- Impact improvement recommendation engine
- Comprehensive impact documentation

---

### Custom Indicator Analysis & Interpretation

#### Custom Indicator Statistical Analysis
**As a data analyst**, I want appropriate statistical analysis guidance for my custom indicators so that I can interpret results with the same rigor as standardized IRIS+ indicators.

**Acceptance Criteria:**
- Statistical method recommendations specific to custom indicator data types
- Benchmarking guidance when no standard comparisons exist
- Trend analysis for custom indicators over time
- Integration of custom indicator results with IRIS+ indicator analysis
- Confidence level and limitation documentation for custom measures

**Technical Requirements:**
- `POST /api/v1/analysis/custom-indicators`
- Custom indicator statistical method recommendation
- Benchmark development for unique indicators
- Trend analysis tools for custom measures
- Integrated analysis frameworks
- Custom indicator limitation documentation

---

#### Custom Indicator Stakeholder Interpretation
**As a program manager**, I want help explaining custom indicator results to stakeholders who may not understand non-standard measures so that our unique indicators enhance rather than confuse our impact story.

**Acceptance Criteria:**
- Plain language explanations of custom indicator methodology
- Comparison frameworks between custom and standard indicators
- Stakeholder-specific interpretation guidance
- Visual representation tools for custom indicator results
- Templates for defending custom indicator validity

**Technical Requirements:**
- Custom indicator explanation templates
- Comparative analysis tools (custom vs. IRIS+)
- Stakeholder communication customization
- Custom indicator visualization tools
- Validity defense documentation

---

## ðŸ“š PHASE 5: Methodology-Driven User Experience
*Guide users through proven practices with progressive complexity*

### Phase-Gated Workflow

#### Foundation-Required Progression
**As any user**, I want the system to prevent me from jumping to indicator selection without completing foundational work so that I avoid the common trap of measuring without context.

**Acceptance Criteria:**
- System blocks indicator access until theory of change is complete
- Progressive revelation of measurement complexity based on organizational readiness
- Clear messaging about why foundation work matters for measurement success
- Foundation completeness scoring with improvement suggestions
- Visual progress through methodology phases

**Technical Requirements:**
- `GET /api/v1/workflow/phase-gate-status`
- `POST /api/v1/workflow/unlock-next-phase`
- Phase-gated workflow enforcement
- Foundation completeness assessment
- Progressive complexity revelation
- Methodology adherence tracking

---

#### Organizational Maturity-Based Guidance
**As an organizational leader**, I want measurement guidance appropriate to our organizational maturity so that we build measurement capability progressively without overwhelming our team.

**Acceptance Criteria:**
- System assesses organizational measurement maturity
- Complexity recommendations matched to organizational capacity
- "Start simple, build up" approach for measurement beginners
- Advanced features unlocked as organizations demonstrate readiness
- Maturity-appropriate language and expectations

**Technical Requirements:**
- `GET /api/v1/organizations/maturity-assessment`
- `POST /api/v1/recommendations/maturity-appropriate`
- Organizational measurement maturity scoring
- Complexity filtering based on capacity
- Progressive feature unlocking system
- Maturity-appropriate content delivery

---

### Transparent Stakeholder Communication

#### Methodology Transparency Documentation
**As a program manager**, I want to document and share our measurement methodology transparently so that stakeholders understand and trust our impact claims.

**Acceptance Criteria:**
- Automated methodology documentation generation
- Plain language explanations of analytical approaches
- Limitation and bias acknowledgment templates
- Stakeholder-appropriate technical detail levels
- Open sharing of methods and data (where appropriate)

**Technical Requirements:**
- `GET /api/v1/organizations/methodology-documentation`
- Methodology documentation automation
- Plain language translation tools
- Stakeholder-appropriate detail customization
- Open data sharing protocols

---

#### Impact Story Integration
**As a communications manager**, I want to combine quantitative findings with qualitative stories so that I can communicate impact in ways that resonate with different stakeholders.

**Acceptance Criteria:**
- AI suggests story collection aligned with quantitative findings
- Templates for integrating numbers and narratives
- Stakeholder-specific communication formats
- Visual storytelling and data presentation tools
- Feedback integration on communication effectiveness

**Technical Requirements:**
- Story-data integration templates
- Stakeholder communication customization
- Visual storytelling tools
- Communication effectiveness tracking
- Multi-format report generation

---

### Custom Indicator Learning & Sharing

#### Custom Indicator Performance Evaluation
**As an M&E lead**, I want to evaluate how well my custom indicators are performing compared to IRIS+ indicators so that I can decide whether to continue, modify, or retire them.

**Acceptance Criteria:**
- Performance comparison between custom and IRIS+ indicators
- Data collection burden analysis for custom vs. standard indicators
- Stakeholder feedback on custom indicator usefulness
- Decision frameworks for indicator refinement or retirement
- Learning documentation for future custom indicator development

**Technical Requirements:**
- `GET /api/v1/indicators/custom/performance-evaluation`
- Comparative performance analysis tools
- Burden-benefit analysis for custom indicators
- Stakeholder feedback analysis
- Indicator lifecycle decision support
- Learning documentation systems

---

#### Custom Indicator Knowledge Sharing
**As a sector practitioner**, I want to share successful custom indicators with peer organizations so that the broader field can benefit from innovation in measurement approaches.

**Acceptance Criteria:**
- Platform for sharing custom indicators with sector peers
- Documentation templates for custom indicator methodology
- Peer review and validation processes for shared indicators
- Attribution and recognition for custom indicator developers
- Integration pathway from custom to potential IRIS+ inclusion

**Technical Requirements:**
- `POST /api/v1/indicators/custom/share`
- Custom indicator sharing platform
- Methodology documentation automation
- Peer review workflow systems
- Attribution and recognition tracking
- IRIS+ submission pathway integration

---

#### Cross-Organization Custom Indicator Learning
**As an innovation network coordinator**, I want to analyze patterns in custom indicators across organizations so that I can identify emerging measurement needs and best practices.

**Acceptance Criteria:**
- Aggregated analysis of custom indicator trends across organizations
- Identification of common gaps not covered by IRIS+
- Best practice synthesis for custom indicator development
- Recommendation engine for new IRIS+ indicators based on custom indicator patterns
- Learning reports for the impact measurement field

**Technical Requirements:**
- Cross-organization custom indicator analysis
- Gap pattern identification algorithms
- Best practice synthesis tools
- IRIS+ recommendation generation from custom patterns
- Field learning report generation

---

## ðŸ”„ CROSS-CUTTING FEATURES

### Foundation-Informed AI Assistance

#### Context-Aware Recommendations
**As any system user**, I want all AI recommendations to be informed by my organization's theory of change and context so that suggestions are always relevant to our specific situation.

**Acceptance Criteria:**
- All indicator suggestions filtered by organizational context
- Data collection recommendations consider organizational capacity
- Analysis suggestions appropriate to our evidence standards
- Communication templates reflect our stakeholder relationships
- Continuous context learning and refinement

**Technical Requirements:**
- Context integration across all AI recommendation engines
- Organizational context weighting in algorithms
- Capacity-appropriate suggestion filtering
- Context learning and improvement systems
- Cross-system context consistency

---

#### Methodology Quality Assurance
**As an impact measurement practitioner**, I want the system to guide me toward methodologically sound practices so that our measurement meets professional standards.

**Acceptance Criteria:**
- Best practice guidance integrated throughout workflow
- Quality checks at each phase of measurement process
- Professional standard compliance verification
- Peer review and validation opportunities
- Continuous methodology improvement suggestions

**Technical Requirements:**
- Best practice guidance system
- Quality assurance checkpoints
- Professional standard compliance checking
- Peer validation frameworks
- Methodology improvement tracking

---

## Implementation Priority - Pitfall Prevention Focus

### Phase 1 (Weeks 1-4): Foundation-First + Core Pitfall Prevention
1. **Theory of Change Required Before Metrics** (phase-gated workflow)
2. **Decision Mapping Foundation** ("what decisions will this inform?")
3. **Activity vs Impact Intelligence** (output/outcome/impact warnings)
4. **Proxy Detection System** (identify and suggest alternatives)

### Phase 2 (Weeks 5-8): AI-Powered Quality Assurance
1. **Over-Engineering Prevention** (minimum viable measurement)
2. **Three-Lens Validation** (quantitative + qualitative + expert)
3. **Context-Aware IRIS+ Discovery** (surface right indicators at decision points)
4. **Sector-Specific Pitfall Guidance** (complexity acknowledgment)

### Phase 3 (Weeks 9-12): Learning-Driven Analytics
1. **Contribution vs Attribution Honesty** (realistic impact claims)
2. **Learning Question Generation** (focus on improvement)
3. **Organizational Maturity Assessment** (progressive complexity)
4. **Evidence Strength Assessment** (credible claim guidance)

### Phase 4 (Weeks 13-16): Custom Indicators + Advanced Features
1. **IRIS+ Gap Identification** (custom indicator needs)
2. **Custom Indicator Development** (rigorous methodology)
3. **Measurement Burden Assessment** (simplification recommendations)
4. **Cross-Organizational Learning** (pattern recognition)

---

---

## Success Metrics - Pitfall Prevention

### Foundation-First Adherence
- **95%** complete theory of change before accessing indicators
- **90%** can articulate decisions their measurement will inform
- **85%** demonstrate logical connection between theory and indicators
- **80%** update their theory of change within first 3 months

### Pitfall Prevention Effectiveness
- **80%** receive and heed activity vs impact warnings
- **75%** select balanced portfolio (not just outputs)
- **70%** identify and address proxy metrics
- **65%** build minimum viable measurement (avoid over-engineering)
- **60%** successfully complete foundation with guided support (no existing ToC)

### Learning Culture Adoption
- **80%** use contribution language appropriately
- **75%** generate learning questions from data
- **70%** focus on improvement over proving
- **65%** acknowledge complexity and limitations honestly

### IRIS+ Discovery Advantage
- **85%** find relevant indicators through contextual recommendations
- **80%** report indicators feel relevant to their specific situation
- **75%** discover indicators they wouldn't have found through browsing
- **70%** successfully implement recommended measurement approaches

### User Behavior Learning (Admin Analytics)
- **Track:** Foundation completion pathways (upload vs guided vs hybrid)
- **Track:** Decision evolution patterns over time
- **Track:** Indicator selection patterns by sector/organization type
- **Track:** Pitfall warning effectiveness (warnings shown vs behavior change)
- **Track:** Theory of change update frequency and triggers
- **Track:** User journey drop-off points and friction areas
- **Track:** Feature usage patterns for product optimization

---

---

## ðŸ“Š ADMIN ANALYTICS REQUIREMENTS
*Learning from user behavior at scale*

### Real-Time User Behavior Tracking

#### Foundation Pathway Analytics
**As a product admin**, I want to understand how users complete their foundation setup so that I can optimize the experience and identify friction points.

**Tracking Requirements:**
- Foundation completion rates by pathway (upload/guided/hybrid)
- Time to complete foundation by pathway
- Drop-off points in guided theory of change creation
- Document upload success rates and common parsing issues
- Foundation update frequency and triggers

**Technical Requirements:**
- `GET /api/admin/analytics/foundation-pathways`
- `GET /api/admin/analytics/completion-funnels`
- Event tracking for all foundation interactions
- A/B testing framework for guided conversation flows

#### Pitfall Prevention Effectiveness
**As a product admin**, I want to measure how effectively our AI prevents measurement pitfalls so that I can improve our guidance algorithms.

**Tracking Requirements:**
- Warnings shown vs warnings heeded by pitfall type
- Indicator selection patterns before/after warnings
- User correction behavior when prompted about proxies or activity measures
- Success rate of minimum viable measurement recommendations
- Sector-specific pitfall occurrence patterns

**Technical Requirements:**
- `GET /api/admin/analytics/pitfall-prevention`
- `GET /api/admin/analytics/warning-effectiveness`
- Warning interaction event tracking
- Behavior change measurement post-warning

#### IRIS+ Discovery Pattern Analysis
**As a product admin**, I want to understand how users discover and select IRIS+ indicators so that I can improve our contextual recommendation engine.

**Tracking Requirements:**
- Most discovered indicators by sector/intervention type
- Contextual recommendation acceptance rates
- Search vs recommendation selection patterns
- Indicator relevance feedback correlation with organizational characteristics
- Custom indicator creation patterns (gaps in IRIS+ coverage)

**Technical Requirements:**
- `GET /api/admin/analytics/iris-discovery-patterns`
- `GET /api/admin/analytics/recommendation-effectiveness`
- Indicator interaction heat mapping
- Contextual relevance scoring analysis

#### Learning Culture Adoption Metrics
**As a product admin**, I want to track how well users adopt learning-focused measurement practices so that I can refine our methodology guidance.

**Tracking Requirements:**
- Language pattern analysis (proving vs improving terminology)
- Learning question generation usage and quality
- Contribution vs attribution language adoption
- Theory of change evolution patterns
- Decision mapping sophistication over time

**Technical Requirements:**
- `GET /api/admin/analytics/learning-culture-adoption`
- `GET /api/admin/analytics/methodology-maturity`
- Natural language analysis of user inputs
- Methodology adherence scoring
- User maturity progression tracking

### Organizational Learning Intelligence

#### Cross-Organizational Pattern Recognition
**As a product admin**, I want to identify patterns across organizations so that I can improve recommendations and identify emerging measurement needs.

**Analytics Requirements:**
- Successful measurement approaches by organization type/sector
- Common theory of change patterns and variations
- Indicator selection clusters and effectiveness
- Organizational maturity progression pathways
- Emerging custom indicator needs across sectors

**Technical Requirements:**
- `GET /api/admin/analytics/cross-org-patterns`
- `GET /api/admin/analytics/emerging-needs`
- Cross-organizational pattern recognition algorithms
- Anonymous organizational comparison analytics
- Trend identification for IRIS+ framework enhancement

#### Product Optimization Intelligence
**As a product admin**, I want real-time insights into user experience friction so that I can rapidly improve the platform based on actual usage patterns.

**Analytics Requirements:**
- User journey flow analysis with drop-off identification
- Feature usage heatmaps and engagement patterns
- Error rate tracking and resolution success
- Performance impact on user behavior
- Feedback sentiment analysis and action item identification

**Technical Requirements:**
- `GET /api/admin/analytics/user-experience-friction`
- `GET /api/admin/analytics/feature-optimization`
- Real-time user behavior event streaming
- Friction point automated identification
- Performance correlation with user success metrics

---

Last Updated: December 2024
Version: 5.0 - Learning-Driven Design with Admin Analytics